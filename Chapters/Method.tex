\chapter{Method}

This chapter presents the data used for training and evaluating models, the models we will test and how the text is processed to be of appropriate format for the models. 

The financial indices we considered in this project are the S\&P 500 and the US treasury rate with 1 and 3 years to maturity:
\begin{itemize}
    \item S\&P 500 (\textit{Standard \& Poor's}) is a stock market index reflecting the performance of the 500 largest companies listed on the stock exchanges in the United States.
    \item The US Treasury rate with maturity $n$ is the yield on a government zero coupon bond with the same maturity. 
\end{itemize}

\section{Data Collection and Pre-Processing}

All models are trained on two datasets -- a labeled movie review dataset from IMDb for benchmarking and an unlabeled financial news dataset from Reuters for the financial interpretation. 

\subsection{IMDd Dataset - Benchmarking}
The IMDb dataset is a commonly used benchmarking dataset for binary classification of text data. The dataset was compiled by \citet{maas-EtAl:2011:ACL-HLT2011} and contains 50,000 polarized movie reviews with 25,000 positive and negative reviews respectively. Evaluating models on this dataset can validate that the models can capture some meaningful information from text data and thus give better insight about the performance on the financial task. 

\subsection{Reuters Financial News Dataset}

The corpus used for this project consists of 109,110 financial news articles from Reuters. It was first compiled and used by \citet{ding2014using} for predicting stock price direction. Only the titles of the news data is used to train the model, as suggested by \citet{ding2014using}. The reasoning behind this is that including the contents of the articles does not significantly improve the model. 

The data contains financial news regarding the US market from the 10th of October 2006 to the 11th of November 2013, with approximately 5-50 news articles per day and is publicly available\footnote{Available at \url{https://github.com/duynht/financial-news-dataset}.}.

\subsubsection{Pre-Processing}

The first part of the pre-processing of text data is similar for all models. The intersection of dates available with news data and financial data is assessed and headlines for these days are extracted. The text is quite clean since it is the headlines of published news articles. It does not require as much cleaning as for instance a corpus of tweets. 

The next step of the pre-processing is to tokenize the data. This is done differently depending on what model to use, but a parameter that is set for all models is the \textit{vocabulary size}. The vocabulary size is simply the amount of words to keep track of. Keras has a tokenization functionality which includes words based on frequency -- the most common words are included. How the tokenization is performed for each model is explained more carefully under \textcolor{red}{each section. MAKE SURE THESE SECTIONS ARE REASONABLE} 

For some of the used models the natural language processing part can be seen as pre-processing, for instance when pre-trained word embeddings are used. This is explained further under each model.

\section{Text Vectorization}
Some text vectorization techniques are jointly trained with the classification model, and thus not possible to isolate from the model. Others can be trained both jointly and used as a pre-processing layer for another model. This project uses three vectorization techniques. In all of the techniques except for Sentence-BERT, the news titles for one day are concatenated in one document representing the news of this day. 

The news dataset contains 27,617 unique words when converted to lower case. 

\subsection{TF-IDF}
Term Frequency--Inverse Document Frequency (TF-IDF) is purely a pre-processing technique which outputs large dimensional vectors of the same length as the vocabulary size. This can then be fed into any model. As stated in the previous chapter, this technique does not preserve the order of words, which does not make it suitable to use as input to a recurrent network. We used a tokenizer from the scikit-learn library to transform the texts into TF-IDF vectors. 

Even though TF-IDF deals with frequency of words, there is still a computational benefit of having a restricted vocabulary, since this is equal to the input dimension. An appropriate vocabulary size is found by evaluating the validation accuracy for different sizes using logistic regression. This is because logistic regression is optimized quickly and can give an indication of when important information for classifying is lost. For the IMDb dataset, this was found to be 10,000 words. 

\subsection{GloVe}

For the word embeddings, we used the GloVe pre-trained word vectors. The vectors have been trained on a dump of Wikipedia from 2014 containing 1.6 billion tokens and the English Gigaword fifth edition dataset, containing 4.3 billion tokens \citep{pennington2014glove}. 400,000 uncased words with an embedding size of 300 dimensions are represented in the used GloVe dictionary. Of the 22,415 words in the news data, 22,107 were available as pre-trained GloVe embeddings. 

The tokenization for the GloVe models first converted all words to lower case and reformed some words such as \textit{won't} and \textit{can't} to \textit{will not} and \textit{can not}. This allows for more words to be initialized with pre-trained embeddings since contracted words are handled differently by different tokenizers. The tokenization is done using the tokenizer from Keras.  

For non-sequential models, the data is converted from $w \times d$ dimensions to $1 \times d$, where $w$ is the number of words in the concatenated news titles for one day and $d$ is the embedding dimensions, here 300. This is done by taking the element-wise average over all $d$ dimensions. In the models using the embeddings as non-trainable pre-processing, this is simply done before the data is fed into the model. In the models where the training of the embeddings are continued, a custom layer in Keras computes the element-wise average after the embedding layer, where the data has been inputted as token indices and the pre-trained embeddings are set in the embedding layer.

For sequential models, the order of words in the titles is kept intact. The number of words in the news titles for one day is shown in Figure~\ref{fig:num_words}. In order for each text sample to have the same size, a maximum sentence length is set. Longer sequences are cut of, shorter sequences are padded with zeros. The maximum sentence length is set to 800 for the news data, which well covers the majority of the samples. 

\begin{figure}[ht]
\centering
\includegraphics[width=0.8\textwidth]{Figures/wordcount.jpg}
\caption{Histogram of the number of words in the concatenated news titles for one day. }
\label{fig:num_words}
\end{figure}

\subsection{Sentence-BERT}

We used a pre-trained BERT model from tensorflow-hub, implemented as a Keras-layer. No pretraining is performed, but the final layers are fine-trained during the training by setting the parameters in the final encoder layers as trainable. This model takes is computationally demanding, so hyperparameter testing is restricted. 


\section{Models}

\subsubsection{Random Classifier}

For an imbalanced rather small dataset, a random classifier can provide a basic understanding of what performance to beat to ensure some predictive power in a model. The random classifier is trained on the training set to respect the ratios of class labels, and then randomly predicts with the same distribution of classes on the validation/test data. 

The random classifier used in this project only relies on the target variables, so it is not interesting to classify it as sequential or not.  

\subsection{Tree-based \& Feed Forward Models}

These models are only used with non-sequential data as input, i.e. TF-IDF, average GloVe embeddings and Sentence-BERT. These vectorizations typically have dimensions 5,000--20,000 (TF-IDF), 300 (GloVe) and 768 (Sentence-BERT). 


\subsubsection{Logistic Regression}
We used logistic regression both as a traditional benchmark and a more novel model in combination with GloVe and BERT embeddings. We used the logistic regression model from the python library scikit-learn  for the implementation.

\subsubsection{Support Vector Machine}
We used a support vector machine (SVM) with similar motivation as for the logistic regression -- it serves as a well known benchmark model and is rather simple to implement and optimize.  We used a support vector machine for binary classification from the scikit-learn library. 

\subsubsection{Feed-forward Network}
A densely connected feed-forward network is evaluated with varying number of hidden layers, nodes and dropout rate. The loss function is binary cross entropy. The model is trained using optimization algorithms RMSprop and Adam, evaluated by cross validation as shown in \textcolor{red}{appendix X}.

The input to the model is either sequential data into an embedding layer (see Figure~\ref{fig:emb_ff}) -- if the embeddings are continuously trained -- or as pre-processed data with static embeddings. For the latter case, this is just a regular multi layer perceptron with rectified linear unit activation function and dropout regularization. The output layer is a single node with a sigmoid activation function.

The model taking sequential inputs has an embedded layer as the first layer. The weights in this layers are set to the pre-trained GloVe embeddings, and are then jointly trained with the other parameters in the model. A maximum sequence length is specified and an average is taken element-wise over all the 300 embedding dimensions. The average is taken where the sum of the absolute value of the embedding vector is non-zero, i.e. the positions in the sequence vector that actually contain a word.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.3\textwidth]{Figures/figs-ff_emb.pdf}
    \caption{Schematic overview of the used feed-forward network with embedded inputs. $b$ is the batch size, $m$ is the maximum sequence length, $d$ is the embedding dimension and $d_n$ is the number of nodes in the dense layer.}
\label{fig:emb_ff}
\end{figure}

This tensor is then passed into a regular multi layer perceptron as described previously.

The feed-forward network is implemented using the Keras-library. 

\subsection{Recurrent Models}

These sequential models all allow further training of embedding parameters, since the first input layer is a regular embedding layer. The input dimensions are $w \times d$, where $w$ is the maximum sequence length and $d$ is the dimension of the embedding. 

\subsubsection{Bidirectional LSTM/GRU}

The bidirectional LSTM/GRU has the same input structure as the feed-forward network, but rather than an averaging layer over all dimensions, an LSTM layer is applied. This preserves the sequential structure of the inputs and enables the model to take the order of words into account. 

The LSTM/GRU layer is wrapped in a bidirectional layer, which implies the input sequences are processed both from left-to-right and from right-to-left. Since this configuration takes a lot of time to train, hyper parameters optimization is performed by manually testing a few parameters.  

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.4\textwidth]{Figures/figs-bidir-lstm.pdf}
    \caption{Schematic overview of a bidirectional LSTM network with sequential input data. $b$ is the batch size, $m$ is the maximum sequence length, $d$ is the embedding dimension and $l_n$ is the number of nodes in the LSTM-layer. The same structure holds for the GRU-architecture.}
\end{figure}


\subsection{Transformer Models}



\begin{figure}[ht]
    \centering
    \includegraphics[width=0.4\textwidth]{Figures/figs-bert-ff.pdf}
    \caption{Schematic overview over the architecture of the BERT model. $b$ is the batch size, $m$ is the maximum sequence length, $d_b$ is the BERT embedding dimension and $d_n$ is the number of nodes in the dense layer.}
\end{figure}


\section{Hyperparameter Optimization and Prevention of Overfitting}

The selection of hyperparameters was carried out using either a grid search or a random search over a selected range of parameters. Grid search was used for random forest and random search was used for other models where hyperparameter optimization was made. Due to the small size of the dataset, a 3-fold cross validation was carried out. 

The tested ranges and concluded sets of parameters for all models are listed in \textcolor{red}{appendix X.}