\chapter{Theoretical Background}

\section{Time Series Analysis}

Traditional time series analysis is concerned with finding statistical information of observations distributed in time. The purpose can be to get a better understanding of the underlying process, or to make predictions on future realisations of the process. Such processes are typically present in the fields of finance, signal processing, weather forecasting, control engineering etc. Several methods for modelling processes have been developed, where some of the oldest and most useful methods are the \textit{autoregressive} model (AR) and the \textit{moving average} model (MA). These two concepts can be combined to benefit from both models into an ARMA-model.

\subsection{Autoregressive (AR) Model} \label{sub:ar}
An auto-regressive model of order $p$ is simply a linear combination of the $p$ previous terms plus some noise. An AR(p)-process is commonly characterized by its generating polynomial $A(z) = a_0 + a_1z + \cdots + a_pz^p$, where $a_i \in \mathbb{R} \ \forall \ i$ and $a_0=1$. Furthermore, let $e_t$ be uncorrelated white noise with variance $\sigma^2$ in discrete time defined as, 
\begin{align}
    \text{E}[e_t] &= 0 \label{eq:wne}\\
    C[e_s, e_t] &= \begin{cases}
    \sigma^2 \ \text{if} \ s=t \\
    0 \ \text{else}
    \end{cases} \label{eq:wnc}
\end{align}

Supposing $A(z)$ is a stable polynomial of degree $p$ and $e_t$ is as defined above, the stationary sequence $X_t$ is called an AR(p)-process with generating polynomial $A(z)$. 
\begin{align}
    X_t + a_1 X_{t-1} + \cdots + a_p X_{t-p} = e_t
    \label{eq:ar}
\end{align}

The process is stable if the roots of the characteristic equation $z^p A(z^{-1}) = 0$ are all inside the unit circle. The values $e_t$ are the called the innovations to the process and the coefficients of the $A(z)$-polynomial are tuneable parameters \citep{lindgren2014stationary}. 

There are several techniques for estimating the coefficients of the $A(z)$-polynomial. One technique is to transform the problem onto regression form and view the $A(z)$-coefficients as regression coefficients. The most recent value $X_t$ is set as the response variable $y_t$ and the previous samples $(-X_{t-1},\dots,-X_{t-p})$ are the explanatory variables $\mathbf{x_t}$. Letting the coefficients $(a_0, a_1,...,a_p)^T = \mathbf{A} $, the expression \ref{eq:ar} can be rewritten on a vector form, recognized from regression. 

\begin{align}
    y_t = \mathbf{x_t} \mathbf{A} + e_t
\end{align}

The elements of $\mathbf{A}$ can then be estimated as the least squares estimate over $n$ samples, i.e. the values of $\mathbf{A}$ that minimize \ref{eq:leastsq}. 

\begin{align}
    L(\mathbf{A}) = \sum_{t=p+1}^n (y_t - \mathbf{x_t} \mathbf{A})^2 \label{eq:leastsq}
\end{align}

Or more conveniently, in matrix format. 

\begin{align}
    \mathbf{Y} = \begin{pmatrix}
    y_{p+1} \\
    y_{p+2} \\
    \vdots \\
    y_n
    \end{pmatrix} , \hspace{1em} \mathbf{E} = \begin{pmatrix}
    e_{p+1} \\
    e_{p+2} \\
    \vdots \\
    e_t
    \end{pmatrix} , \hspace{1em} \mathbf{X} = \begin{pmatrix}
    \mathbf{x}_{p+1} \\
    \mathbf{x}_{p+2} \\
    \vdots \\
    \mathbf{x}_{t}
    \end{pmatrix} \\[10pt] 
    L(\mathbf{A}) = (\mathbf{Y} - \mathbf{X}\mathbf{A})^T(\mathbf{Y} - \mathbf{X}\mathbf{A})
    \label{eq:leastsqmat}
\end{align}

Differentiating \ref{eq:leastsqmat} with respect to $\mathbf{A}$ and setting equal to zero yields the least squares estimate. 

\begin{align}
    \frac{\partial L}{\partial \mathbf{A}} &= -2 \mathbf{X}^T \mathbf{Y} + 2\mathbf{X}^T \mathbf{X} \mathbf{A} \\
    \frac{\partial L}{\partial \mathbf{A}} &= 0 \implies \hat{\mathbf{A}} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{Y}
 \end{align}

The estimated innovation variance $\hat{\sigma^2}$ is simply the sample variance of the innovations.

\begin{align}
    \hat{\sigma^2} =\frac{1}{n-p} \sum_{t=p+1}^n e_t^2 = \frac{L(\hat{\mathbf{A}})}{n-p}
\end{align}

The method above yields an estimate for an AR(p)-process. The order p of the process still has to be determined using appropriate goodness-of-fit criteria such as Akaike Information Criterion \citep{lindgren2014stationary}. 


\textcolor{red}{Borde jag ha med Yule-Walker ekvationer h√§r kanske? }

\subsection{Moving Average (MA) Model}

Another popular time series model is the moving average (MA) model. This model is similarly to the AR-model defined by its generating polynomial, here called $C(z)$. 

\begin{align}
    C(z) = c_0 + c_1 z + \cdots + c_q z^q 
\end{align}

A MA(q)-process is a linear combination of the q previous white noise terms as defined in \ref{eq:wne}, \ref{eq:wnc}, plus one new innovation term. 

\begin{align}
    X_t = e_t + c_1 e_{t-1} + \cdots + c_q e_{t-q}
\end{align}

A common adjustment to the model is to set $c_0 = 1$ and adjust the other coefficients and the innovation variance accordingly. 

An important distinction between the AR(p)-process and the MA(q)-process that the covariance function for the MA(q)-process is zero for time lags $\tau$ larger than the order q of the process. In other words, the value $X_{t+q + 1}$ is independent of the value $X_t$ in an MA(q)-process \citep{lindgren2014stationary}. 

\subsection{Autoregressive Moving Average (ARMA) Model}


The AR-process and MA-process are commonly combined into an ARMA-process. Letting the noise on the right hand side of \ref{eq:ar} be an MA(q)-process, the expression of an ARMA(p,q)-process is achieved. 

\begin{align}
    X_t + a_1 X_{t-1} + \cdots + a_pX_{t-p} = e_t + c_1 e_{t-1} + \cdots + c_q e_{t-q}
    \label{eq:arma}
\end{align}

There are various methods for estimating the coefficients of the polynomials $A(z)$ and $C(z)$. One is a regression approach similar to the method described in \ref{sub:ar}, where regression samples are constructed from the time series and the coefficients are estimated in a least squares sense. Another common estimation procedure is to use maximum likelihood estimation of the parameters. Assuming the noise follows some distribution - most commonly Gaussian - a distribution can be calculated for the model coefficients. The likelihood of obtaining a given set of a parameters can then be calculated and optimized \citep{hamilton1994time}. 

\section{Representation of Language}

Language has enabled people to exchange information in an efficient manner for thousands of years, both through speech and text. The complexity and nuances that makes a language so fitting for transferring information between humans is also what makes it so difficult to represent in numbers. A few of the difficulties when interpreting text language are listed below. 
\begin{enumerate}[i)]
    \item \textit{Homonyms}, words that have the same spelling but different meaning. Consider for instance the word \textit{"bull"}, which might refer to the animal or an investor who believes in a rising market. The same letters, but with vastly different interpretations depending on the context. 
    \item \textit{Negations}. The sentences \textit{"God will help you"} and \textit{"No God will help you"} contain almost the same words, but have completely opposite meanings. 
    \item \textit{Sarcasm}. This can be hard enough to detect for humans. The phrase "That's just what I needed today!" might actually mean what it literally says, or just the opposite.
\end{enumerate}

Methods used for representing words deal with these difficulties in different ways or not at all. For some shallow, more simple NLP-tasks, a model might be perform well without understanding homonyms or that two words are closely related. For more complex tasks such as sequence-to-sequence translation of language, a deeper understanding is naturally needed. 

\subsection{One-hot Encoding}

If a group of students were asked to construct a way of turning  sequences of words into numerical data, one could imagine that one-hot encoding would be what they would come up with. One-hot encoding is simply done by giving all unique words in a text an index and then letting the index represent the word. Consider a training example $x_i$ as the sentence below. 

\begin{center}
    \textit{A gorilla visited Manilla.}    
\end{center}

The first processing needed is to divide the sentence into smaller units - \textit{tokens}. A tokenized version of the sentence above would be,

\begin{center}
    \textit{"A", "gorilla", "visited", "Manilla", "."}    
\end{center}

Some of these tokens carries information about the beginning or end of the sentence, which is obviously important. However, not all capital letters imply the start of a sequence and not all punctuation indicates the end of a sequence, e.g. \textit{"Hello Mr. Gorilla!"}. There are quite a few special cases of this sort, and there are convenient functions in python that deal with the problems of tokenization, such as \textit{Tokenizer} from Keras \citep{chollet2020keras}. The tokenizer from Keras splits the text into sequences, removes the punctuation and transforms all characters to lower case by default. 

When the sentence is tokenized, it is also common to remove the most frequent words, since these probably doesn't give a lot of information about the difference between sentences. These are called \textit{stop words} and typically include common words such as \textit{a, the, but} etc.

After the tokenization, each unique word is given an index. 

\begin{center}
    \begin{tabular}{rc}
        \textit{gorilla} $:$ & 1 \\
        \textit{visited} $:$ & 2 \\
        \textit{manilla} $:$ & 3 \\
    \end{tabular}
\end{center}

Each word of the sentence is then transformed to a one-hot vector where all elements are zero except for element $i$. A sequence of words can then be represented as a sequence of vectors. 

\begin{align*}
    \textit{gorilla} : \begin{bmatrix}
    1 & 0 & 0 
    \end{bmatrix} \\
    \textit{visited} : \begin{bmatrix}
    0 & 1 & 0 
    \end{bmatrix} \\
    \textit{manilla} : \begin{bmatrix}
    0 & 0 & 1 
    \end{bmatrix} \\
\end{align*}


\subsection{Bag-of-words \& TF-IDF}

An initial bag-of-words (BOW) approach to represent a sentence as a vector is to simply keep track of whether a word is included in the sentence or not. If the word is included, the element on the corresponding index has value 1, otherwise 0. The previously used example would then be represented by $x_i$ as, 


\begin{align*}
    x_i = \begin{bmatrix}
    1 & 1 & 1 & 1
    \end{bmatrix}
\end{align*}

Note that even though the order of words is the same in the vector as in the original sentence, this is not necessarily the case. The less interpretable sentence \textit{Manilla visited a gorilla} would have the same vector representation as $x_i$. Hence, BOW does not take order of words in a sentence into account. 

There are variations of BOW that have larger representing power, such as including the count of words in the sentence rather than if it exists or not. A problem with this approach is that words that are more frequent in sentences gets a higher value than words that are not as frequent, even if less frequent words might be more interesting for the context. A variation that deals with this limitation is the TF-IDF representation. 

Term Frequency-Inverse Document Frequency (TF-IDF) is a widely used technique for normalizing text data. It uses the same underlying principles as BOW but with a weight normalization. As the name suggests, the value for a certain word is increased for its frequency in a document but decreased for the frequency in the whole document which is considered. So, a word which has a low frequency in a full corpus is considered more important than a word with low frequency. The entry $j$ in a vector $x_i$ is then calculated as the product of the term frequency weight and the inverse document frequency weight. 

\begin{align*}
    x_{ij} = f_s(t,f) \cdot f_d(t,F)
\end{align*}

For a term $t$ with frequency $f$ in sequence $i$ and frequency $F$ in the whole document. The function $f_s$ is some function increasing with the number of words $j$ in the sentence and $f_d$ is decreasing with the number of words in the full document. Examples of these function can be as below. 

\begin{align*}
    f_s &= \frac{\mid \{ j \in (1, \hdots , L_i) : s_{ij} = t  \} \mid}{L_i} \\
    f_d &= \log{\frac{N}{n_t}}
\end{align*}

Where $L_i$ is the length of the sentence $i$, $N$ is the number of sequences in the document and $n_t$ is the number of documents in which the term $t$ occurs at least once \citep{manning2008introduction}. 

While this remedies some of the shortcomings of BOW, there are still some aspects where it falls short. 

Firstly, the size of the vectors grow with the number of unique words in the corpus, which an become computationally infeasible for larger texts. 

Secondly, the order of the words is not accounted for in BOW. When used in an application for interpreting financial news, the representation must be able to distinguish between \textit{"Google place a bid on Amazon"} and \textit{"Amazon place a bid on Google"}. BOW however simply registers the occurrence of words. 

Finally, BOW does not really capture any essence of the language. There is no way for the mode to capture the similarity between words such as \textit{awesome} and \textit{amazing}. This is related to the large dimensionality of the vectors representing the words, since each word has a unique dimension in the vector. This implies all dimensions are orthogonal, therefore there is no usable algebraic measure of similarity. 


All of the problems above are addressed by the concept of word embeddings in the next section. 




\subsection{Word Embeddings}

As opposed to the sparse representation of one-hot encoding, \textit{word embeddings} are dense, continuous vector representations of words. The dimension of a one-hot encoded vector is proportional to the size of the vocabulary (generally 20,000 or greater), whereas word embedding vectors typically have 100 to 1000 dimensions \citet{chollet2017deep}.

A major break through on the topic of word embeddings was made by \citep{mikolov2013efficient} where efficient methods of training a model were introduced, popularised as the \textit{word2vec}-model. Two model architectures for training the $d$-dimensional vector representations of words are described, \textit{continuous bag-of-words} and \textit{continuous skip-gram}. Both of the methods share the  notion that the meaning of a word is determined by which words it is commonly used together with. The representations are learned by constructing fake tasks which are then solved by a neural network. This task is generally to predict neighboring words of a given word. The weights in the trained neural network is the interesting component.

\textit{Continuous bag-of-words} is trained by predicting the missing word in a sequence of words with a given window size. The order of the words are not taken into consideration other than for deciding which words to include in one sequence. For instance, the phrase \textit{"A gorilla visited Manilla."} with window size one gives the following training samples.
\begin{center}
\textit{"A gorilla visited"} $\implies x = $ \textit{("a", "visited")} $,y = $ \textit{"gorilla"} \\ \vspace{1em}
\textit{"gorilla visited Manilla"} $\implies x = $ \textit{("gorilla", "Manilla")} $,y = $ \textit{"visited"} \\
\end{center}

Again, note that the order of the words above are not taken into account. 


\textit{Continuous skip-grams} also use the fact that words that often occur together have some sense of similarity, but is in a way the inverse of continuous bag-of-words. Rather than predicting the missing word, the objective of the model is to predict the surrounding words. To construct training examples from the same phrase as above with a 1-skip-gram, the following samples are generated. 

\begin{center}
\textit{"A gorilla visited"} $\implies (x,y) = $ \textit{("gorilla", "a"), ("gorilla", "visited")} \\ \vspace{1em}
\textit{"gorilla visited Manilla"} $\implies (x,y) = $ \textit{("visited", "gorilla"), ("visited", "Manilla")}
\end{center}

The known words above are \textit{"gorilla"} and \textit{"visited"} respectively. 

According to the authors\footnote{See for instance: \url{https://code.google.com/archive/p/word2vec/}}, the continuous bag-of-words model is faster for training but the continuous skip-gram model is better for infrequent words. 


The word embeddings generated by these methods does carry some information about the semantic relationship of words. As mentioned in the previous section, a desirable function of word representation is to determine whether a word is close to another word in a semantic meaning. This is elegantly represented in word embeddings as the cosine similarity between word vectors. Consider for instance the word \textit{Sweden}. The closest word vectors in the word2vec-vocabulary with respect to cosine similarity are displayed in Table~\ref{tab:cossim}.

\begin{table}[h!]
    \centering
    \begin{tabular}{lr}
    \hline
        \textbf{Word} & \textbf{Cosine similarity}  \\
        \hline \hline 
        Finland & 0.8085 \\
        Norway  & 0.7706 \\
        Denmark & 0.7674 \\
        Swedish & 0.7404 \\ 
        Swedes  & 0.7133 \\
        \hline 
    \end{tabular}
    \caption{Word vectors with the highest cosine similarity to \emph{Sweden}. Pre-trained embeddings from the word2vec module of the python gensim library were used.}
\label{tab:cossim}
\end{table}

There is also a straight forward interpretation of elementwise addition and subtraction of these word embeddings. In some sense, the $d$-dimensions of the embeddings can be interpreted to be metrics of different properties. For instance, the sum of the embeddings for the words \emph{doctor} and \emph{animal} is most similar to the embedding for the word "veterinarian". There is also reasonable syntactic results when performing simple mathematical operations. It would for instance be expected that the difference between \emph{running} and \emph{run} is similar to the difference between \emph{swimming} and \emph{swim}. This can roughly be expressed as below. 
\begin{align}
    \emph{running - run} \approx  \emph{swimming - swim}
\end{align}

Indeed, taking the embeddings for the words and calculating \emph{running - run + swim} results in a vector which is most similar to the embedding for the word \emph{swimming} using the 44,000 most common words in the python gensim implementation of word2vec.


\section{Classification Models}

The text vectorization methods presented in the previous section enables all classification models that can take a vector as input as possible options for text classification tasks. In these cases, the text vectorization is seen as a step of input pre-processing rather than a part of the model. The focus is mainly on Section \ref{sec:recmods} with models where word representations are jointly trained with the model, or where the models are specifically appropriate for natural language processing tasks. However, a few more general models are briefly mentioned in Section \ref{sec:treeff}. 

\subsection{Tree-based \& Feed Forward Models}\label{sec:treeff} 

\subsubsection*{Random Forest}

A random forest classifier is an ensembling technique, using several decision trees and then taking the mode or the average of the output from all of the decision trees. Since a decision tree is prone to overfitting to the training data, taking the average over many trees gives a more robust prediction less sensitive to what training data it is presented with. Random forest has been a widely popular model which tends to work well without much tuning out of the box for shallow machine learning tasks \citep{chollet2017deep}. 

\subsubsection*{Logistic Regression}

Logistic regression is a commonly used baseline for machine learning classification tasks. Even though it is an old model, it can still provide good results for emerging fields of classification tasks \citep{chollet2017deep}. 

\subsubsection*{Support Vector Machines}

Support vector machines are a group of models which can be used for both classification and regression. The core idea of a classification support vector machine is to find a suitable decision boundary between data points of different classes. This decision boundary is a hyper plane in some dimension depending on the number of features in the input data. The input data is mapped to a higher dimensional representation where the samples are easier to separate. The best decision boundary is then found by maximizing the distance between the closest samples and the hyper plane in the high dimensional representation. Support vector machines can quickly make predictions on unseen data since the new sample only has to be evaluated against the decision boundary consisting of the hyper plane in the high dimensional representation \citep{chollet2017deep}.   



\subsubsection*{Multi Layer Perceptron}

A multi layer perceptron, or feed forward network, is a neural network that seeks to approximate some function $f$. For a classifier, this function would map features $x$ to some prediction $\hat{y}$ of the true value $y$. The prediction depends on the weights and biases $w,b$ of the network.  


\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{Figures/figs-perceptron.pdf}
    \caption{A single perceptron unit with two inputs $x_1, x_2$, two weights $w_1, w_2$ and one bias $b$. The sigmoid in the prediction node is an activation function usually denoted $\sigma(x)$.}
    \label{fig:perceptron}
\end{figure}

The core part of the multi layer perceptron is the single perceptron unit displayed in Figure~\ref{fig:perceptron}. it is easy to see that the prediction $\hat{y}$ is a function of the inputs, weights and biases as $\hat{y} = \sigma(x_1 w_1 + x_2 w_2 + b)$, where $\sigma$ is the activation function denoted by a sigmoid in the figure. The training of the model implies selecting the weight and biases such that the predictions are as accurate as possible. The bias term $b$ is usually included in the term "weights" for convenience, since it can be seen as a weight for the input value $x_0 = 1$. The accuracy of classification is measured by some loss function $J$, which depends on the training samples $(x,y)$ and the weights $w$.  A common loss function for binary classifcation is the binary crossentropy, defined (\ref{eq:bce}), where $y$ is the true label, $\hat{y}$ is the predicted label and $p(\hat{y})$ is the predicted probability of the sample being of label 1. Conversely, $1 - p(\hat{y})$ is the probability of the sample being of label 0. 

\begin{align}
    L(y,\hat{y}) = -\frac{1}{N} \sum_{i=1}^N y_i \log{(p(\hat{y}_i))} + (1 - y_i) \log{(1 - p(\hat{y}_i))} \label{eq:bce}
\end{align}

A single perceptron unit is not too exciting, but when combined into a network it is more powerful. \citet{leshno1993multilayer} proved that a neural network with a locally bounded piecewise continuous activation function can approximate any continuous function to any degree of accuracy if and only if the activation function is not a polynomial. This is known as the universal approximation theorem. While this implies that a sufficiently large multi layer perceptron is able to represent any continuous function, it does not guarantee that a network is able to learn this representation. Furthermore, the size of the network is unrestricted and can in practice be infeasible computationally \citep{Goodfellow-et-al-2016}. 


\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{Figures/ff.PNG}
    \caption{Overview of a densely connected feed forward network with six inputs, two hidden layers and one output node.}
    \label{fig:mlp}
\end{figure}

Combining the basic functionality in Figure~\ref{fig:perceptron} with the network in Figure~\ref{fig:mlp}, a neural network is achieved. This can function as both a regression and classification model. For regression, the activation function in the final node is typically a linear function as $\varphi(x) = cx$, for some real constant $c$. For classification, it is usually some sigmoid function bounded between 0 and 1, such as $\displaystyle{\varphi(x) = \frac{1}{1 + e^{-x}}}$. The most popular activation function in the nodes which are inside the network is the rectified linear unit function (ReLU), defined as $\varphi(x) = \max(0,x)$ \citep{Goodfellow-et-al-2016}. These functions are all conveniently implemented in keras, as well as a structure using different layers to build a model \citep{chollet2020keras}. 

\subsection{Recurrent Models}\label{sec:recmods}

A drawback which includes all of the models presented in the previous section is that the order of words are not taken into account. Of course, the order can be considered in the pre-processing if some more complex transformation is used (e.g. Sentence-BERT), but these transformations usually utilize some recurrent or attention based method. 

\subsubsection{Recurrent Neural Networks}

A recurrent neural network (RNN) is rather similar to a multi layer perceptron, but it has a method for taking previous inputs into account. The input to an RNN is a sequence of samples, typically of length equal to a sentence where each entry is a word index. One argument for why this makes sense is that when a human is reading a sentence, the previous words are remembered and of large importance for how the sentence is interpreted. We keep the a general idea of the meaning of what we have previously read and adjust the idea depending on the new words in the sentence. Recurrent neural networks work in a rather similar way, storing hidden representations of inputs and conveying that information to the next input in the sequence \citep{Goodfellow-et-al-2016}. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/figs-simplernn.pdf}
    \caption{General graph of a sequence-to-sequence RNN unfolded through time, where a sequence of inputs $x$ predicts a sequence of outputs $y$. U, V and W are weights that are shared throughout the sequence, independent of the position.}
    \label{fig:rnn}
\end{figure}

As seen in Figure~\ref{fig:rnn}, an RNN can be seen as a feed-forward neural network where previous time steps are used as input. Assuming a hidden layer activation function $\sigma_h$ and an output activation function $\sigma_o$, the recurrent nature of the model can be further explained. Let the other notation be as in Figure~\ref{fig:rnn}. 

\begin{align}
    \hat{y_t} &= \sigma_o(V \cdot h^{(t)}) = \sigma_o(V \cdot  \sigma_h(U x^{(t)} + W h^{(t-1)})) \\
    h^{(t-1)} &= \sigma_h(Ux^{(t-1)} + W h^{(t-2)})
\end{align}

Clearly, the prediction $\hat{y}^{(t)}$ is dependent on the previous inputs $x^{(i)}$ and hidden states $h^{(i)}$. 

The model structure is similar for sequence-to-one classifications, i.e. a sequence $(x_i^{(1)}, x_i^{(2)}, ..., x_i^{(l)})$ predicts one label $\hat{y}_i$. Given a sequence length $l$, only the final step $x_i^{(l)}$ provides an output $y_i$. This output is however influenced by the previous inputs $x_i^{(t)}$ through the hidden layers $h^{(t)}$ and the weights $W$, see Figure~\ref{fig:rnns2o}. 


\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/figs-rnn-s2o.pdf}
    \caption{Schematic graph of a sequence-to-one RNN unfolded through time, where a sequence of inputs $(x^{(1)}, ..., x^{(l)})$ predicts one output $y$.}
    \label{fig:rnns2o}
\end{figure}

\subsubsection{Long Short-Term Memory (LSTM)}

Even though the general idea of the previously described method is widely used, the model is not used precisely as described. The simple RNN-structure suffers from the \textit{vanishing gradient problem}. To understand this problem one must understand how the network is trained, which is outside the scope of this summary. In short, long term dependencies are too many time-steps away to have an impact when the network is trained. In practice, either Long-Short-Term Memory (LSTM) or Gated Recurrent Unit (GRU) is used instead. The main difference is that information that is deemed important is allowed to pass on to later time-steps without too much interference from hidden several dot products and activation functions. This alleviates the vanishing gradient problem, even though the core idea is the same as the previously described simple RNN \citep{chollet2017deep}.

\subsubsection*{Bidirectional LSTM}

As a final note for the recurrent neural network approach, it should be mentioned that when RNNs are used in natural language processing, they are often wrapped with a bidirectional layer. This simply reverts the input sequence and enters the sequence in both the original and the reverse direction to two separate RNNs, usually LSTM or GRU. 

The usefulness of this is particularly intuitive when looking at the network in Figure~\ref{fig:rnn}. When processing the entry $x^{(j)}$, only the entries $t < j$ are known. However, tokens later in the sequence might have an impact on the previous outputs of the model. Bidirectional LSTMs can catch patterns that are overlooked by regular LSTMs.

Bidirectional LSTMs have been widely popular in the field of natural language processing. In 2017, the model behind Google Translate was powered by seven stacked LSTM layers \citep{chollet2017deep}. 

\subsection{Transformers}

In \citeyear{NIPS2017_7181}, \citeauthor{NIPS2017_7181} wrote a paper, \textit{Attention is all you need}, which had a considerable influence on language modelling and machine translation. The authors presented a new model - the Transformer - which does away with the recurrence and instead only uses the concept of  attention. One of the problems with recurrent networks is the lack of parallelizability when training the network. The Transformer allows for more parallelization than previous models and consequently trains quicker than models based on recurrence. This is proved by a new state of the art score of 28.4 BLEU on the WMT 2014 English-to-German translation task which was trained at a fraction of the training cost compared to the previous state of the art models. 


In the rest of the section, I will follow \citet{NIPS2017_7181} as well as a more detailed description by \citet{jayalammar2018} to describe the principles behind the Transformer. The explained model is used for sequence-to-sequence modelling such as translation, but is also the foundation for BERT which can be used for classification. This is the motivation for this rather detailed section on transformers. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/figs-transformer.pdf}
    \caption{Schematic graph of a Transformer.}
    \label{fig:trfo}
\end{figure}

The core idea is to encode the input into a manageable, general representation and then decode that representation into the sought output. Both the encoder and decoder part consist of 6 identical layers of encoders and decoders.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{Figures/The-Transformer-model-architecture.png}
    \caption{Model architecture of the transformer with the encoder to the left and the decoder to the right. The paper uses $N = 6$ layers for both the encoder and the decoder. After \citet{NIPS2017_7181}.}
    \label{fig:tr_arch}
\end{figure}

\subsubsection{Transformer Tokenization}

Firstly, the input to the model is tokenized in a different way than what has been described for previous models. It is similar in the sense that each token is transformed into an embedding, \citet{NIPS2017_7181} uses a WordPiece embeddings of dimension $d_{model} = 512$. The positional encoding is however quite different. 

Rather than letting a sequential input to the model represent the position of tokens similar to an RNN, the positional information is included in sine and cosine functions with different frequencies. 

\begin{align}
    PE(pos,i) = \begin{cases}
    \sin\Big(\frac{pos}{10000^{2i/d_{model}}}\Big) \ , & i = 2k \\
    \cos\Big(\frac{pos}{10000^{2i/d_{model}}}\Big) \ , & i = 2k + 1 
    \end{cases} \   k \in \mathbb{N}
    \label{eq:posid}
\end{align}

With the notation in \ref{eq:posid} being $pos$ for the position of the token in the sentence and $i$ the dimension of the embedding. One word has one position $pos$ and $d_{model} = 512$ dimensions.

This way of representing the positions of a word might seem cumbersome at first, but it has some nice features. 

\begin{description}
    \item[Dimensional consistency.] These positional embeddings can be chosen to have the same dimension as the vectors used for the word embedding. In the original papers, the positional encodings are simply added to each element $i$ of the $d_{model} = 512$ dimensions. 
    \item[Generalizable.] This method can extrapolate to sentence lengths not seen in the training data. 
    \item[Simple relative positioning.] The fact that $PE(pos, i)$ can be written as a linear function of $PE(pos, i + k)$ for any $k$, should according to \citet{NIPS2017_7181} make it easier for the model to acknowledge relative positions. 
    \item[No recurrence.] This is not unique for this sinusoidal version of positional encoding, but is however one of the main features of the Transformer, allowing more parallel computations. 
\end{description}

\subsubsection{Encoder \& Self-Attention}

After the input embedding and positional encoding, a sample is passed through to the stack of encoder layers, shown to the left in Figure~\ref{fig:tr_arch}. This is where the concept of self-attention comes in. 

At a conceptual level, self-attention for words could be described as how much other words in the sentence represent the current word, i.e. what other words in the sentence the word pays attention to. As an example, consider the following two sentences. 

\begin{center}
    \textit{"The gorilla didn't like Manilla, it was too crowded."} \\ \vspace{1em}
    \textit{"The gorilla didn't like Manilla, it was too tired."}
\end{center}

In order to interpret the sentence correctly, a model must understand that "it" refers to "Manilla" in the first sentence and to "the gorilla" in the second. This is the purpose of the self-attention, to detect which words in the neighborhood that are important for the meaning of the current word. The mathematics of it all is further explained below. 

As seen in Figure~\ref{fig:tr_arch}, there are 6 layers of multi-head attention with following feed forward layers. One multi-head attention is made up of 8 identical parallel self-attention layers. Each attention layer has unique weights. In the first layer, the inputs are simply word embeddings and positional encodings. Consider the sentence \textit{"A gorilla visited Manilla."}, with some word embeddings $x$ and positional encodings $PE$. The vector which is given to the self-attention layer is then $h^o$ as in \ref{eq:attinp}. Note that the dimension for $x, PE$ and $h^o$ is $l \times d_{model}$, where $l = 4$ in this example and $d_{model} = 512$ in the original paper \citep{NIPS2017_7181}. 

\begin{align}
    x &= \begin{bmatrix} x_1 & x_2 & x_3 & x_4 \end{bmatrix}^\intercal \\
    PE &= \begin{bmatrix} PE_1 & PE_2 & PE_3 & PE_4
    \end{bmatrix}^\intercal \\
    h^o &= \begin{bmatrix} x_1 + PE_1 & x_2 + PE_2 & x_3 + PE_3 & x_4 + PE_4 \end{bmatrix}^\intercal \label{eq:attinp}
\end{align}

The first calculations in the attention layer are three linear transformations to construct three vectors: a query vector, a key vector and a value vector. The matrices $W_Q, W_K$ and $W_V$ are weights which are tuned during the training and determines the query, key and value vectors. 

\begin{align}
    q_i = h_i^o W_q \\
    k_i = h_i^o W_k \\
    v_i = h_i^o W_v 
\end{align}

These vectors are used to calculate an attention score for a given word $i$ versus all other words $j$ in the sentence. The score is calculated as the dot product of $q_i$ and $k_j$ and scaled by $\displaystyle{\frac{1}{\sqrt{d_k}}}$ where the original paper uses a dimension of $d_k=64$ for the query, key and value vectors.

\begin{align}
    \text{score}_{i,j} = \frac{q_i \boldsymbol{\cdot} k_j}{\sqrt{d_k}}
\end{align}

The scaling by $d_k$ is made to keep the gradients of the attention layer at a more manageable level. The scores for the word $i$ against all other words $j$ are then normalized with a softmax function, ensuring the sum over all words $j$ sum up to one and that larger scores are boosted. The softmaxed score for word $i$ against word $j$ is then calculated as in \ref{eq:softmax}.

\begin{align}
    \text{softscore}_{i,j} = \frac{e^{\text{score}_{i,j}}}{\sum\limits_{\forall j} e^{\text{score}_{i,j}}}
    \label{eq:softmax}
\end{align}

Finally, the contribution to the attention for each word is calculated by adding up all the value vectors $v_j$ weighted by the softmaxed score of $i$ and $j$. This concludes the calculation of the new hidden state $h_i^{o+1}$ for word $i$. 

\begin{align}
    h_i^{o+1} = \sum\limits_{\forall j} v_j \cdot  \text{softscore}_{i,j} 
\end{align}

A flow graph over how this is calculated is shown in Figure~\ref{fig:attflow}. 

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{Figures/figs-self-att.pdf}
    \caption{Calculation of self-attention from hidden layer $o$ to $o+1$ for word $i$. "mul" implies matrix multiplication or dot product. }
    \label{fig:attflow}
\end{figure}

The details of the calculation until now has concerned one word at the time in the sentence. It can however be neatly compressed into matrix form.

Letting $h^o$ be as defined in \ref{eq:attinp}, we can  write the query, key and value vectors as matrices where every row corresponds to the vector for a word in the sentence. 

\begin{align}
    Q = h^o W^Q \\
    K = h^o W^K \\
    V = h^o W^V
\end{align}

The attention of the layer can then be calculated as in \ref{eq:attout}, by the steps shown for one word in the previous segments. 

\begin{align}
    \text{Attention}(Q,K,V) = \text{softmax}\Big( \frac{Q K^\intercal}{\sqrt{d_k}}\Big) V
    \label{eq:attout}
\end{align}

What has been demonstrated until now is the flow for a sample through one attention layer. However, the Transformer model uses a multi-head attention layer. This is simply 8 attention layers that runs in parallel. The weights across the layers are not shared, but the idea is that having eight attention layers with random initialization enables focusing on different aspects of language. The output of the eight layers are concatenated and multiplied by another matrix $W^O$ of weights in order to reduce the dimension to $l \times d_{model}$, which is the same dimensions as was fed to the model in \ref{eq:attinp}. 

Note that the first inputs to the model are word embeddings and positional encodings, but the rest of the encoder layers has the output from the previous layer as input, which is of the same dimension as the input embeddings. 

The final step in the encoder layer is a position-wise feed forward network, roughly as seen in Figure~\ref{fig:mlp}. The network has one hidden layer of size $d_{ff} = 2048$. The input and output are both of size $d_{model} = 512$. The difference to a regular multi layer perceptron is that the weights are applied identically to each position, greatly reducing the number of tuneable weights. The hidden layer has ReLU activation functions, so the output for any input can be conveniently written as in \ref{eq:attmlp}

\begin{align}
    F(x) = \max(0, x W_1 + b_1)W_2 + b_2)\label{eq:attmlp}
\end{align}

Another important property is the residual connections seen in Figure~\ref{fig:tr_arch}. These allow information to transfer to a new layer without being distorted in the attention calculations. The authors \citet{NIPS2017_7181} claim this is particularly important for the positional encodings to stay intact deeper into the network. 



\subsubsection{Decoder \& Output}

There are many similarities between the encoder and decoder, but some things differ. The output from the encoder is used to calculate attention vectors $K$ and $V$ which are used to predict a sequential output from the decoder. The first input to the decoder are these attention vectors and a start of sequence-token. These then progress through the decoder layers and produce an output. The output is made by a linear transformation to a logits-vector where each entry is an index in a vocabulary. A softmax function is applied, and the entry in the vector with the highest values is chosen as the prediction. The predicted word is then transformed with word embedding and positional encoding as in the input, and the next word in the sequence is predicted using the previous word as input. The calculated attention vectors $K$ and $V$ are the same for all of the positions in the decoder. 

One large difference to the encoding part is that the self-attention layers are only allowed to attend to previous inputs, i.e. words to the left. This is done by setting the entries for words to the right of the currently processed word to $-\infty$ in the dot product of $Q$ and $K$ before the softmax as seen in Figure~\ref{fig:attflow}. This ensures no attention is given to latter words in the sentence in the decoding part. 

\subsubsection{BERT}

The Transformer presented by \citeauthor{NIPS2017_7181} was influential for several other models. \citeauthor{devlin2018bert} presented the language representation model BERT in \citeyear{devlin2018bert}. BERT is short for Bidirectional Encoder Representations from Transformers. This model achieved new state-of-the-art results on eleven NLP tasks. The success of BERT is largey due to the fact that it can be used for a wide variety of NLP tasks with only small changes to model architecture and further training.

A lot of the mechanics of BERT is included in the encoder part of the transformer seen to the left in Figure~\ref{fig:tr_arch}. The bidirectionality in the model is represented in the same way as the self-attention in the encoder of the transformer. As mentioned in the explanation of the transformer, a word in a sentence is allowed to attend to words to the right. This is not the case for the decoder, where words to the right are masked out. The bidirectionality in BERT is thus not quite the same as the bidirectionality in an LSTM or GRU, since the latter simply reverses the input sequence to achieve bidirectionality. In BERT, the sequences are handled in a more parallel sense, as the positions are considered only as the positional embeddings - not by the position in the sequence. 

\textcolor{red}{PRE-TRAINS IN TWO WAYS: MASKED LANGUAGE MODELLING AND NEXT SENTENCE PREDICTION}

BERT is trained on the BooksCorpus (800M words) and the English Wikipedia (2.500M words). 

BERT can be used as a feature extractor for classification tasks by the [CLS]-token seen in Figure~\ref{fig:berttokens} 

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{Figures/bert_inputrep.png}
    \caption{BERT input visualization for a sentence pair, A \& B. After \citet{devlin2018bert}.}
    \label{fig:berttokens}
\end{figure}


\subsubsection{Sentence-BERT}



\section{Overfitting \& Hyperparameter Optimization}


\section{Performance Metrics}
The metrics used to evaluate the models are accuracy, recall, precision and F1-score. While these metrics are all somewhat related, they give different insights of performance. For convenience a binary confusion matrix is displayed in Figure~\ref{fig:confmat}. 

\begin{figure}[h]
\centering
\begin{tabular}{cccc}
     \multirow{2}{*}{Actual values} & 0 & \multicolumn{1}{|c|}{True Negatives (TN)} &  \multicolumn{1}{c|}{False Negatives (FN)} \\
     \cline{2-4}
      & 1 & \multicolumn{1}{|c|}{False Positives (FP)} & \multicolumn{1}{c|}{True Positives (TP)} \\ 
      \cline{2-4}
     & & \multicolumn{1}{|c|}{0} & \multicolumn{1}{c|}{1}\\
     
     & & \multicolumn{2}{c}{Predictions} \\
\end{tabular}
\caption{Confusion matrix for binary classification.}
\label{fig:confmat}
\end{figure}

\begin{description}
    \item[Accuracy:] The most intuitive measure of performance, simply the ratio of correct classifications to the total number of classifications. If the labels of a data set are symmetric, this is a good measure of performance. However, if 90 \%  of the samples are of label $a$ and 10\% of label $b$, a model which constantly predicts label $a$ gets an accuracy of 90\%. It doesn't reveal any information about which data is classified incorrectly. 
    \item[Precision:] The number of correctly classified samples in one class divided by the total number of predictions of that class. The precision $P$ for class 0 in Figure~\ref{fig:confmat} is calculated as $\displaystyle{P = \frac{TN}{TN + FP}}$. 
    \item[Recall:] The number of correctly classified samples in one class divided by the total number of samples in that class. In other words, a measure of how many percent of a class that was found. In relation to Figure~\ref{fig:confmat} it is calculated as $\displaystyle{R = \frac{TN}{TN + FN}}$. Recall is also referred to as \textit{sensitivity} in statistical literature. 
    \item[F1-score:] A measure which combines precision and recall as the harmonic mean of the two. Calculated as $\displaystyle{F1 = 2 \frac{R \cdot P}{R + P}}$. An F1-score of 1 implies perfect recall and precision.
\end{description}

The precision, recall and F1-score are metrics which are calculated for every class in a classification problem. In order to get a single metric for a set of samples, these can be weighted by the number of true labels of each class. Hence for a binary classification problem, $F1_w = F1_0 \cdot w_0 + F1_1 \cdot w_1$, where $w_i$ is the ratio of labels in class $i$ in the training set \citep{Ting2017}. 

