\chapter{Results}\label{chap:results}

In this section, the results on the benchmark dataset are briefly presented followed by the results on the financial data.
The results from the movement of indices are displayed in Section \ref{sec:res_dir}. Finally, the results from the prediction of the ARMA-forecasts are presented in Section \ref{sec:res_arma}. Again, this classification problem aims to predict if the ARMA-forecast is higher or lower than the actual outcome. 

The performance is presented as accuracy and macro average F1-score on the test set for all models. 

\newpage
\section{Benchmark Evaluation}

The results on the IMDb dataset for benchmarking are presented in Table~\ref{tab:imdb}. While most of the models trained rather quickly, it is worth mentioning that BERT and the bidirectional LSTM both needed > 24 hours to be optimized on the validation set.
BERT with a multi-layer perceptron performed the best on this dataset. It is however closely followed by SVM, logistic regression and multi-layer perceptron with features generated by SBERT, trained at a fraction of the time needed for BERT.
Another note is that the GloVe embeddings performed quite poorly in the pretrained setting, but had a large improvement when the embeddings were continuously trained. 

\begin{table}[t]
    \centering
    \begin{tabular}{llr}
    \hline
        \multicolumn{2}{c}{\textbf{Model}} & \textbf{Acc} \\
        \hline \hline   
        \multirow{4}{*}{TF-IDF} & Random forest & 85.6 \\
        & SVM & 89.5 \\
        & Logistic regression & 89.1 \\
        & MLP & 89.2 \\
        \hline 
        \multirow{4}{*}{GloVe (pretrained)} & Random forest & 80.7 \\
        & SVM & 84.1 \\
        & Logistic regression & 85.2 \\
        & MLP & 84.1 \\
        \hline 
        \multirow{2}{*}{GloVe (jointly trained)} & MLP & 88.2 \\
        & Bidirectional LSTM & 90.1 \\
        \hline 
        \multirow{4}{*}{Sentence-BERT} & Random forest & 87.1 \\
        & SVM & 90.0 \\
        & Logistic regression & 90.7 \\
        & MLP & 90.1 \\
        \hline
        BERT & MLP & \textbf{91.0}  \\
        \hline
    \end{tabular}
\caption{Performance of used methods on the IMDb dataset. }
\label{tab:imdb}
\end{table}

\newpage 
\section{Index Direction Predictions}\label{sec:res_dir}

This section presents the results from predicting the direction of an index on a daily basis. Two experiments have been made. The first one uses the news from day $k$ to predict the movement from day $k-1$ to $k$. The second uses the news from day $k$ to predict the movement from day $k$ to $k+1$. Naturally, the first task should be easier since more information is available. It is however not trivial, since the targets are whether the median average of the bid and ask rate over the full day has increased or decreased. It is dependent on the market movement of the whole day, while the news titles are published continuously throughout the day.  
Confusion matrices are displayed for a selection of models in Figures \ref{tab:confmat_cdp} and \ref{tab:confmat_ndp}.

The dataset contains 1,846 days, partitioned into 1,200 samples for training and validation and 646 samples for testing. The label distribution is presented in Table \ref{tab:label_dist1}. The distribution was selected to be the same in the training and test set. 

% Label distribution
\begin{table}[h]
    \centering
    \begin{tabular}{lrr}
        & \textbf{0} & \textbf{1} \\
        \hline \hline 
        1 year rate & 0.61 & 0.39 \\
        3 year rate & 0.54 & 0.46 \\
        S\&P 500 & 0.55 & 0.45 \\
        \hline 
    \end{tabular}
    \caption{Label distribution for the three series in the index prediction task. }
    \label{tab:label_dist1}
\end{table}

% Results CD
\begin{table}[h]
    \centering
    \begin{tabular}{llrrrrrr}
    \hline
        \multicolumn{2}{c}{\multirow{2}{*}{\textbf{Model}}} & \multicolumn{2}{c}{\textbf{1 year}} & \multicolumn{2}{c}{\textbf{3 year}} & \multicolumn{2}{c}{\textbf{S\&P}} \\
      & & \textbf{F1} & \textbf{Acc} & \textbf{F1} & \textbf{Acc} & \textbf{F1} & \textbf{Acc} \\
        \hline \hline   
        \multirow{2}{*}{Random classifier} & Most frequent & \textbf{0.56} & \textbf{68.9} & 0.44 & 59.4 & 0.37 & 53.6 \\
        & Stratified & \textbf{0.56} & 57.4 & 0.50 & 53.3 & 0.50 & 50.8 \\
        \hline 
        \multirow{4}{*}{TF-IDF} & Random Forest & 0.41 & 68.7 & 0.42 & 58.4 & 0.55 & \textbf{59.6} \\
        & SVM & 0.41 & \textbf{68.9} & 0.40 & 59.4 & 0.51 & 57.3 \\
        & Logistic Regression & 0.41 & 68.9 & 0.43 & 58.4 & 0.55 & 58.0  \\
        & MLP & 0.41 & 68.4 & 0.37 & 59.4 & \textbf{0.56} & 57.7 \\
        \hline 
        \multirow{4}{*}{GloVe (pretrain)} & Random forest & 0.42 & 67.6 & 0.40 & 58.5 & 0.55 & 58.2 \\
        & SVM & 0.41 & \textbf{68.9} & 0.37 & 59.4 & 0.35 & 53.6 \\
        & Logistic regression & 0.41 & 68.6 & 0.40 & 59.0 & 0.53 & 57.3 \\
        & MLP & \textbf{0.56} & \textbf{68.9} & 0.45 & 59.3 & 0.48 & 56.5 \\
        \hline 
        \multirow{2}{*}{GloVe (jointly trained)} & MLP & 0.41 & \textbf{68.9} & 0.37 & 59.4 & 0.47 & 55.7 \\
        & Bidirectional LSTM & 0.42 & 67.5 & 0.37 & 59.1 & 0.49 & 49.2  \\
        \hline 
        \multirow{4}{*}{Sentence-BERT} & Random forest & 0.41 & 66.6 & 0.51 & \textbf{60.4} & \textbf{0.56} & 56.8 \\
        & SVM & 0.41 & \textbf{68.9} & 0.37 & 59.4 & 0.54 & 57.0 \\
        & Logistic regression & 0.53 & 67.2 & \textbf{0.55} & 59.4 &\textbf{0.56} & 57.1  \\
        & MLP & 0.41 & \textbf{68.9} & \textbf{0.55} & 58.4 & \textbf{0.56} & 56.8 \\
        \hline
        \multirow{2}{*}{BERT} & Sigmoid & 0.41 & \textbf{68.9} & 0.38 & 59.4 & 0.48 & 50.2 \\
        & MLP & 0.41 & \textbf{68.9} & 0.37 & 59.4 & 0.38 & 54.3 \\ 
        \hline
    \end{tabular}
\caption{Performance of used methods on classifying whether the price of an index has increased from day $k-1$ to $k$ given the news titles from day $k$. }
\label{tab:rescdp}
\end{table}
% Confmat CD
\begin{figure}[h]
    \centering
    \begin{tabular}{cccc}
     & & \multicolumn{2}{c}{Predictions} \\
     & & \multicolumn{1}{|c|}{0} & \multicolumn{1}{c|}{1}\\
      \cline{2-4}
     \multirow{2}{*}{Labels} & 0 & \multicolumn{1}{|c|}{92} &  \multicolumn{1}{c|}{208} \\
      & 1 & \multicolumn{1}{|c|}{53} & \multicolumn{1}{c|}{293} \\ 
      \\ \multicolumn{4}{c}{TF-IDF + random forest} \\ \multicolumn{4}{c}{S\&P 500}
\end{tabular} \hspace{1em}
\begin{tabular}{cccc}
     & & \multicolumn{2}{c}{Predictions} \\
     & & \multicolumn{1}{|c|}{0} & \multicolumn{1}{c|}{1}\\
      \cline{2-4}
     \multirow{2}{*}{Labels} & 0 & \multicolumn{1}{|c|}{442} &  \multicolumn{1}{c|}{3} \\
      & 1 & \multicolumn{1}{|c|}{201} & \multicolumn{1}{c|}{0} \\ 
      \\ \multicolumn{4}{c}{TF-IDF + MLP}
      \\ \multicolumn{4}{c}{S\&P 500}
\end{tabular} \hspace{1em}
\begin{tabular}{cccc}
     & & \multicolumn{2}{c}{Predictions} \\
     & & \multicolumn{1}{|c|}{0} & \multicolumn{1}{c|}{1}\\
      \cline{2-4}
     \multirow{2}{*}{Labels} & 0 & \multicolumn{1}{|c|}{143} &  \multicolumn{1}{c|}{157} \\
      & 1 & \multicolumn{1}{|c|}{120} & \multicolumn{1}{c|}{226} \\ 
      \\ \multicolumn{4}{c}{SBERT + logreg}
      \\ \multicolumn{4}{c}{S\&P 500}
\end{tabular}
    \caption{Confusion matrices for some models for the current day prediction task presented in Table \ref{tab:rescdp}.}
    \label{tab:confmat_cdp}
\end{figure}
% Results ND
\begin{table}[t]
    \centering
    \begin{tabular}{llrrrrrr}
    \hline
        \multicolumn{2}{c}{\multirow{2}{*}{\textbf{Model}}} & \multicolumn{2}{c}{\textbf{1 year}} & \multicolumn{2}{c}{\textbf{3 year}} & \multicolumn{2}{c}{\textbf{S\&P}} \\
      & & \textbf{F1} & \textbf{Acc} & \textbf{F1} & \textbf{Acc} & \textbf{F1} & \textbf{Acc} \\
        \hline \hline   
        \multirow{2}{*}{Random classifier} & Most frequent & 0.41 & 68.9 & 0.37 & 59.4 & 0.35 & 53.6 \\
        & Stratified & \textbf{0.50} & 55.3 & \textbf{0.49} & 52.2 & 0.49 & 51.5 \\
        \hline 
        \multirow{4}{*}{TF-IDF} & Random forest & 0.41 & 68.9 & 0.40 & 58.4 & 0.54 & \textbf{59.1} \\
        & SVM & 0.41 & 68.9 & 0.38 & 58.4 & 0.49 & 56.3 \\
        & Logistic regression & 0.41 & 68.9 & 0.40 & 55.9 & \textbf{0.55} & 58.2 \\
        & MLP & 0.41 & 68.4 & 0.47 & 55.9 & 0.47 & 55.9 \\
        \hline 
        \multirow{4}{*}{GloVe (pretrain)} & Random Forest & 0.41 & 67.6 & 0.43 & 58.4 & 0.52 & 53.1 \\
        & SVM & 0.41 & 68.9 & 0.37 & 59.4 & 0.35 & 53.6 \\
        & Logistic Regression & 0.41 & 68.9 & 0.44 & 59.6 & \textbf{0.55} & 58.2 \\
        & MLP & 0.41 & 68.9 & 0.37 & 59.4 & \textbf{0.55} & 55.4 \\
        \hline 
        \multirow{2}{*}{GloVe (jointly trained)} & MLP & 0.41 & 68.9 & 0.37 & 59.4 & 0.35 & 53.6 \\
        & Bidirectional LSTM & 0.41 & 68.9 & 0.38 & 58.5 & 0.48 & 48.6 \\
        \hline 
        \multirow{4}{*}{Sentence-BERT} & Random forest & 0.43 & \textbf{69.2} & 0.44 & \textbf{59.9} & 0.50 & 52.9 \\
        & SVM & 0.41 & 68.9 & 0.37 & 59.4 & 0.36 & 54.0  \\
        & Logistic regression & 0.47 & 66.6 & 0.51 & 56.3 & 0.53 & 54.3  \\
        & MLP & 0.41 & 68.9 & 0.38 & 59.8 & 0.37 & 53.1 \\
        \hline
        \multirow{2}{*}{BERT} & Sigmoid & 0.41 & 68.9 & 0.40 & 58.7 & 0.47 & 53.3 \\
        & MLP & 0.41 & 68.9 & 0.37 & 59.1 & 0.39 & 45.5 \\ 
        \hline
    \end{tabular}
\caption{Performance of used methods on classifying whether the price of an index has increased from day $k-1$ to $k$ given the news titles from day $k-1$.}
\label{tab:resndp}
\end{table}
% Confmat ND
\begin{figure}[t!]
    \centering
    \begin{tabular}{cccc}
     & & \multicolumn{2}{c}{Predictions} \\
     & & \multicolumn{1}{|c|}{0} & \multicolumn{1}{c|}{1}\\
      \cline{2-4}
     \multirow{2}{*}{Labels} & 0 & \multicolumn{1}{|c|}{83} &  \multicolumn{1}{c|}{217} \\
      & 1 & \multicolumn{1}{|c|}{47} & \multicolumn{1}{c|}{299} \\ 
      \\ \multicolumn{4}{c}{TF-IDF + random forest} \\
      \multicolumn{4}{c}{S\&P 500}
\end{tabular} \hspace{1em}
\begin{tabular}{cccc}
     & & \multicolumn{2}{c}{Predictions} \\
     & & \multicolumn{1}{|c|}{0} & \multicolumn{1}{c|}{1}\\
      \cline{2-4}
     \multirow{2}{*}{Labels} & 0 & \multicolumn{1}{|c|}{101} &  \multicolumn{1}{c|}{199} \\
      & 1 & \multicolumn{1}{|c|}{71} & \multicolumn{1}{c|}{275} \\ 
      \\ \multicolumn{4}{c}{TF-IDF + logreg} \\
      \multicolumn{4}{c}{S\&P 500}
\end{tabular} \hspace{1em}
\begin{tabular}{cccc}
     & & \multicolumn{2}{c}{Predictions} \\
     & & \multicolumn{1}{|c|}{0} & \multicolumn{1}{c|}{1}\\
      \cline{2-4}
     \multirow{2}{*}{Labels} & 0 & \multicolumn{1}{|c|}{102} &  \multicolumn{1}{c|}{198} \\
      & 1 & \multicolumn{1}{|c|}{73} & \multicolumn{1}{c|}{273} \\ 
      \\ \multicolumn{4}{c}{GloVe + logreg} \\
      \multicolumn{4}{c}{S\&P 500}
\end{tabular}
    \caption{Confusion matrices for some models for the next day prediction task presented in Table \ref{tab:resndp}}
    \label{tab:confmat_ndp}
\end{figure}



As seen in Table \ref{tab:rescdp}, a lot of the results for the one year rate are similar. This is due to the models simply predicting the most frequent class for every sample in the test set. Such a behavior was seen repeatedly, especially for the two treasury rates. The highest performing model on the test set for the three year treasury rate seen in Table \ref{tab:rescdp} is SBERT with random forest, logistic regression and MLP. The predictions are however heavily skewed towards label 0 and can not be assumed to be significantly better than a random classifier.
The best performance over the random baseline is achieved on the S\&P index. Both the F1-score and the accuracy on the test set is improved compared to the baseline for some models. 

The results in Table \ref{tab:resndp} generally show a little lower performance on the test set, across all time series. A random classifier using the `stratified' method achieves the best F1-score. The S\&P series does however show an absolute improvement over the random baseline. TF-IDF and random forest give the highest accuracy on the test set. The highest F1-score is achieved by TF-IDF + logistic regression together with pretrained Glove embeddings  combined with logistic regression or a multilayer perceptron. 

\section{ARMA Direction Predictions}\label{sec:res_arma}

This section presents the results from the ARMA direction prediction task. The label is 0 if the ARMA prediction is higher than the actual outcome. Otherwise it is 1. 

An ARMA(1,1) model is fitted to the time series for every date with available news. The accuracy of the prediction for the entire dataset is shown in Table \ref{tab:arma_acc}. These are mainly shown to give an idea of the accuracy of the ARMA-model, but also gives a baseline for comparison with the task in Section \ref{sec:res_dir}.

\begin{table}[h]
    \centering
    \begin{tabular}{lr}
         \textbf{Index} &  \textbf{Acc}\\
         \hline \hline 
         1 year rate & 65.9 \\
         3 year rate & 56.0 \\
         S\&P 500 & 47.6 \\
         \hline 
    \end{tabular}
    \caption{Accuracy of the ARMA-predictions. }
    \label{tab:arma_acc}
\end{table}

The same structure as in the previous section is applied for this task. One evaluation uses the news from day $k$ to predict the validity of the forecast for day $k$, see Table \ref{tab:res_arma_cdp}. The other evaluation concerns using news from $k-1$ to predict the validity of the ARMA-forecast for day $k$, see Table \ref{tab:res_arma_ndp}. Confusion matrices for the best performing model for every time series are displayed in Tables \ref{tab:confmat_arma_cdp} and \ref{tab:confmat_arma_ndp}.
The labels used in this task are slightly less skewed than the labels in Section \ref{sec:res_dir}, as seen in Table \ref{tab:label_dist2}.

% Label distribution
\begin{table}[h]
    \centering
    \begin{tabular}{lrr}
       \textbf{Index}  & \textbf{0} & \textbf{1} \\
        \hline \hline 
        1 year rate & 0.61 & 0.39 \\
        3 year rate & 0.54 & 0.46 \\
        S\&P 500 & 0.55 & 0.45  \\
        \hline 
    \end{tabular}
    \caption{Label distribution for the three series in the ARMA prediction task. }
    \label{tab:label_dist2}
\end{table}


% Results CD
\begin{table}[H]
    \centering
    \begin{tabular}{llrrrrrr}
    \hline
        \multicolumn{2}{c}{\multirow{2}{*}{\textbf{Model}}} & \multicolumn{2}{c}{\textbf{1 year}} & \multicolumn{2}{c}{\textbf{3 year}} & \multicolumn{2}{c}{\textbf{S\&P}} \\
      & & \textbf{F1} & \textbf{Acc} & \textbf{F1} & \textbf{Acc} & \textbf{F1} & \textbf{Acc} \\
        \hline \hline   
        \multirow{2}{*}{Random Classifier} & Most Frequent & 0.38 & 61.1 & 0.35 & 53.8 & 0.35 & 55.0  \\
        & Stratified & 0.47 & 54.4 & 0.51 & 50.4 & 0.48 & 52.9  \\
        \hline 
        \multirow{4}{*}{TF-IDF} & Random forest & 0.46 & 62.0 & 0.50 & 53.3 & 0.46 & 55.7  \\
        & SVM & 0.46 & 62.3 & 0.49 & 56.6 & 0.45 & 56.6 \\
        & Logistic regression & 0.49 & 62.3 & 0.51 & 55.0 & 0.48 & 54.7 \\
        & MLP & 0.38 & 61.1 &  0.54 & 54.9 & 0.45 & 56.4 \\
        \hline 
        \multirow{4}{*}{GloVe (pretrain)} & Random Forest & 0.52 & \textbf{63.1} & 0.53 & 55.8 & 0.49 & 53.6
        \\
        & SVM & 0.38 & 61.1 & 0.35 & 53.8 & 0.35 & 55.0 
        \\
        & Logistic regression & 0.48 & 61.7 & 0.54 & 55.8 & 0.52 & 57.8
        \\
        & MLP & 0.52 & 62.2 & 0.35 & 53.8 & 0.50 & 55.0 \\
        \hline 
        \multirow{2}{*}{GloVe (jointly trained)} & MLP & 0.38 & 61.1 & 0.35 & 53.8 & 0.36 & 54.6 \\
        & Bidirectional LSTM & 0.39 & 60.9 & 0.50 & 51.5 & 0.48 & 56.4 \\
        \hline 
        \multirow{4}{*}{Sentence-BERT} & Random Forest & 0.55 & 62.3 & 0.53 & 54.6 & 0.59 & 60.9 \\
        & SVM & 0.38 & 61.1 & 0.50 & 55.2 & 0.52 & 58.8 \\
        & Logistic regression & \textbf{0.57} & 61.6 & \textbf{0.57} & \textbf{57.4} & \textbf{0.62} & \textbf{62.6}   \\
        & MLP & 0.51 & 62.5 & \textbf{0.57} & 56.9 & 0.55 & 59.4 \\
        \hline
        \multirow{2}{*}{BERT} & Sigmoid & 0.48 & 60.2 & 0.37 & 52.9 & 0.36 & 54.6 \\
        & MLP & 0.44 & 61.2 & 0.48 & 52.4 & 0.43 & 53.6 \\
        \hline
    \end{tabular}
\caption{Performance of used methods on classifying whether the ARMA-prediction of an index on day $k-1$ was higher or lower than the outcome on day $k$, given the news titles from day $k$.}
\label{tab:res_arma_cdp}
\end{table}

% Confmat CD
\begin{figure}[h]
    \centering
    \begin{tabular}{cccc}
     & & \multicolumn{2}{c}{Predictions} \\
     & & \multicolumn{1}{|c|}{0} & \multicolumn{1}{c|}{1}\\
      \cline{2-4}
     \multirow{2}{*}{Labels} & 0 & \multicolumn{1}{|c|}{357} &  \multicolumn{1}{c|}{37} \\
      & 1 & \multicolumn{1}{|c|}{201} & \multicolumn{1}{c|}{50} \\ 
      \\ \multicolumn{4}{c}{GloVe + random forest} \\
      \multicolumn{4}{c}{1 year rate}
\end{tabular} \hspace{1em}
\begin{tabular}{cccc}
     & & \multicolumn{2}{c}{Predictions} \\
     & & \multicolumn{1}{|c|}{0} & \multicolumn{1}{c|}{1}\\
      \cline{2-4}
     \multirow{2}{*}{Labels} & 0 & \multicolumn{1}{|c|}{225} &  \multicolumn{1}{c|}{122} \\
      & 1 & \multicolumn{1}{|c|}{153} & \multicolumn{1}{c|}{145} \\ 
      \\ \multicolumn{4}{c}{SBERT + logreg} \\
      \multicolumn{4}{c}{3 year rate}
\end{tabular} \hspace{1em}
\begin{tabular}{cccc}
     & & \multicolumn{2}{c}{Predictions} \\
     & & \multicolumn{1}{|c|}{0} & \multicolumn{1}{c|}{1}\\
      \cline{2-4}
     \multirow{2}{*}{Labels} & 0 & \multicolumn{1}{|c|}{252} &  \multicolumn{1}{c|}{103} \\
      & 1 & \multicolumn{1}{|c|}{138} & \multicolumn{1}{c|}{152} \\ 
      \\ \multicolumn{4}{c}{SBERT + logreg} \\
      \multicolumn{4}{c}{S\&P 500}
\end{tabular}
    \caption{Confusion matrices for some models related to the results presented in Table \ref{tab:res_arma_cdp}.}
    \label{tab:confmat_arma_cdp}
\end{figure}

% Results ND
\begin{table}[H]
    \centering
    \begin{tabular}{llrrrrrr}
    \hline
        \multicolumn{2}{c}{\multirow{2}{*}{\textbf{Model}}} & \multicolumn{2}{c}{\textbf{1 year}} & \multicolumn{2}{c}{\textbf{3 year}} & \multicolumn{2}{c}{\textbf{S\&P}} \\
      & & \textbf{F1} & \textbf{Acc} & \textbf{F1} & \textbf{Acc} & \textbf{F1} & \textbf{Acc} \\
        \hline \hline   
        \multirow{2}{*}{Random classifier} & Most frequent & 0.38 & 61.1 & 0.35 & 53.9 & 0.36 & 55.1 \\
        & Stratified & 0.48 & 52.8 & 0.53 & 53.1 & 0.48 & 50.3 \\
        \hline 
        \multirow{4}{*}{TF-IDF} & Random forest & 0.45 & 61.3 & 0.50 & 54.5 & 0.55 & 60.7 \\
        & SVM & 0.42 & 61.5 & 0.43 & 53.7 & 0.50 & 58.0 \\
        & Logistic regression & 0.47 & 61.0 & 0.52 & 56.7 & 0.55 & 60.2 \\
        & MLP & 0.57 & 61.8 & 0.36 & 53.6 & 0.46 & 56.3 \\
        \hline 
        \multirow{4}{*}{GloVe (pretrain)} & Random forest & 0.50 & 60.2 & 0.51 & 52.3 & 0.50 & 57.7 \\
        & SVM & 0.38 & 61.1 & 0.35 & 53.9 & 0.36 & 55.1 \\
        & Logistic regression & 0.52 & 62.2 & 0.54 & 55.7 & 0.48 & 55.6  \\
        & MLP & 0.58 & 60.7 & 0.44 & 55.0 & 0.51 & 56.3 \\
        \hline 
        \multirow{2}{*}{GloVe (jointly trained)} & MLP & \textbf{0.60} & 61.3 & 0.42 & 55.0 & 0.37 & 55.4 \\
        & Bidirectional LSTM & 0.43 & 61.1 & 0.45 & 52.8 & 0.40 & 54.2 \\
        \hline 
        \multirow{4}{*}{Sentence-BERT} & Random forest & 0.56 & 63.6 & \textbf{0.55} & 56.5 & 0.59 & 61.5 \\
        & SVM & 0.38 & 61.1 & 0.53 & \textbf{57.4} & 0.51 & 59.3 \\
        & Logistic regression & 0.58 & 63.2 & 0.54 & 55.0 & \textbf{0.63} & \textbf{64.1} \\
        & MLP & 0.53 & \textbf{63.8} & 0.46 & 55.4 & 0.46 & 57.4 \\
        \hline
        \multirow{2}{*}{BERT} & Sigmoid & 0.48 & 60.9 & 0.35 & 53.5 & 0.50 & 54.3  \\
        & MLP & 0.53 & 58.6 & 0.35 & 53.8 & 0.41 & 54.4 \\ 
        \hline
    \end{tabular}
\caption{Performance of used methods on classifying whether the ARMA-prediction of an index on day $k-1$ was higher or lower than the outcome on day $k$, given the news titles from day $k-1$.}
\label{tab:res_arma_ndp}
\end{table}

% Confmat ND
\begin{figure}[h]
    \centering
    \begin{tabular}{cccc}
     & & \multicolumn{2}{c}{Predictions} \\
     & & \multicolumn{1}{|c|}{0} & \multicolumn{1}{c|}{1}\\
      \cline{2-4}
     \multirow{2}{*}{Labels} & 0 & \multicolumn{1}{|c|}{261} &  \multicolumn{1}{c|}{134} \\
      & 1 & \multicolumn{1}{|c|}{115} & \multicolumn{1}{c|}{136} \\ 
      \\ \multicolumn{4}{c}{GloVe (joint) + MLP} \\
      \multicolumn{4}{c}{1 year rate}
\end{tabular} \hspace{1em}
\begin{tabular}{cccc}
     & & \multicolumn{2}{c}{Predictions} \\
     & & \multicolumn{1}{|c|}{0} & \multicolumn{1}{c|}{1}\\
      \cline{2-4}
     \multirow{2}{*}{Labels} & 0 & \multicolumn{1}{|c|}{244} &  \multicolumn{1}{c|}{104} \\
      & 1 & \multicolumn{1}{|c|}{179} & \multicolumn{1}{c|}{119} \\ 
      \\ \multicolumn{4}{c}{SBERT + random forest} \\
      \multicolumn{4}{c}{3 year rate}
\end{tabular} \hspace{1em}
\begin{tabular}{cccc}
     & & \multicolumn{2}{c}{Predictions} \\
     & & \multicolumn{1}{|c|}{0} & \multicolumn{1}{c|}{1} \\
      \cline{2-4}
     \multirow{2}{*}{Labels} & 0 & \multicolumn{1}{|c|}{258} &  \multicolumn{1}{c|}{98} \\
      & 1 & \multicolumn{1}{|c|}{133} & \multicolumn{1}{c|}{157} \\ 
      \\ \multicolumn{4}{c}{SBERT + logreg} \\
      \multicolumn{4}{c}{S\&P 500}
\end{tabular}
    \caption{Confusion matrices for some models related to the results presented in Table \ref{tab:res_arma_ndp}.}
    \label{tab:confmat_arma_ndp}
\end{figure}

The results in this section are a bit more aligned than in the previous task. SBERT is the feature extraction technique that generally provides the best performance for this task. The performance over the random baseline is highest for the S\&P data. SBERT combined with logistic regression provides the best results for S\&P for both the current day predictions in Table \ref{tab:res_arma_cdp} and the next day predictions in Table \ref{tab:res_arma_ndp}. 



